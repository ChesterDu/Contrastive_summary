{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('contras_sum': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8cb9d025c05246ef6003f562bd78c39f89a3e3de8b2c6293d42fd24001b7bf88"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/newdisk/yangkai/contras_sum\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from scl_data import summarize\n",
    "def get_raw_data(path):\n",
    "    raw_data = []\n",
    "    f = open(path,\"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for line in tqdm.tqdm_notebook(lines):\n",
    "        line = line.strip()\n",
    "        label, text = line.split(',',1)\n",
    "        label = int(label[1])-1\n",
    "        text = text[1:-1]\n",
    "        sum = summarize(text)\n",
    "        raw_data.append((label,text,sum))\n",
    "\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/nesa320/anaconda2/envs/contras_sum/lib/python3.6/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=650000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c75a95a0ffd441590e8fa5375499620"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c1b3bfccef4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dataset/yelp_multi/yelp_review_full_csv/train.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dataset/yelp_multi/yelp_review_full_csv/test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-a3ace5ef2c88>\u001b[0m in \u001b[0;36mget_raw_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/newdisk/yangkai/contras_sum/scl_data.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msentence_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nesa320/anaconda2/envs/contras_sum/lib/python3.6/site-packages/sumy/summarizers/text_rank.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, document, sentences_count)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_best_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nesa320/anaconda2/envs/contras_sum/lib/python3.6/site-packages/sumy/summarizers/text_rank.py\u001b[0m in \u001b[0;36mrate_sentences\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nesa320/anaconda2/envs/contras_sum/lib/python3.6/site-packages/sumy/summarizers/text_rank.py\u001b[0m in \u001b[0;36m_create_matrix\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_as_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_as_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rate_sentences_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# delta added to prevent zero-division error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#(see issue https://github.com/miso-belica/sumy/issues/112 )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nesa320/anaconda2/envs/contras_sum/lib/python3.6/site-packages/sumy/summarizers/text_rank.py\u001b[0m in \u001b[0;36m_rate_sentences_edge\u001b[0;34m(words1, words2)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;31m# This should only happen when words1 and words2 only have a single word.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# Thus, rank can only be 0 or 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misclose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/home/nesa320/anaconda2/envs/contras_sum/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2287\u001b[0m     \u001b[0mxfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2289\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mall\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/home/nesa320/anaconda2/envs/contras_sum/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m     \"\"\"\n\u001b[0;32m-> 2411\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_path = \"dataset/yelp_multi/yelp_review_full_csv/train.csv\"\n",
    "raw_train_data = get_raw_data(train_path)\n",
    "test_path = \"dataset/yelp_multi/yelp_review_full_csv/test.csv\"\n",
    "raw_test_data = get_raw_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save({\"train\":raw_train_data,\"test\":raw_test_data}, \"dataset/yelp_multi/raw_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "raw_dataset = torch.load(\"dataset/yelp_multi/raw_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data = raw_dataset[\"train\"]\n",
    "raw_test_data = raw_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(4,\n",
       "  \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\",\n",
       "  \"he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.\"),\n",
       " (1,\n",
       "  \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\",\n",
       "  \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff. You have office workers, you have patients with medical needs, why isn't anyone answering the phone?\"),\n",
       " (3,\n",
       "  \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\",\n",
       "  'Been going to Dr. Goldberg for over 10 years. He explores all options with you and is very patient and understanding.'),\n",
       " (3,\n",
       "  'Got a letter in the mail last week that said Dr. Goldberg is moving to Arizona to take a new position there in June.  He will be missed very much.  \\\\n\\\\nI think finding a new doctor in NYC that you actually like might almost be as awful as trying to find a date!',\n",
       "  '\\\\n\\\\nI think finding a new doctor in NYC that you actually like might almost be as awful as trying to find a date!'),\n",
       " (0,\n",
       "  \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\",\n",
       "  \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. He will not give refills and could less about patients's financial situations. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call.\"),\n",
       " (4,\n",
       "  \"Top notch doctor in a top notch practice. Can't say I am surprised when I was referred to him by another doctor who I think is wonderful and because he went to one of the best medical schools in the country. \\\\nIt is really easy to get an appointment. There is minimal wait to be seen and his bedside manner is great.\",\n",
       "  'Top notch doctor in a top notch practice.'),\n",
       " (4,\n",
       "  'Dr. Eric Goldberg is a fantastic doctor who has correctly diagnosed every issue that my wife and I have had. Unlike many of my past doctors, Dr. Goldberg is very accessible and we have been able to schedule appointments with him and his staff very quickly. We are happy to have him in the neighborhood and look forward to being his patients for many years to come.',\n",
       "  'Unlike many of my past doctors, Dr. Goldberg is very accessible and we have been able to schedule appointments with him and his staff very quickly.'),\n",
       " (0,\n",
       "  \"I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\\\n\\\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an $800.00 bill for the blood work. I can't pay for my bill because I'm a student and don't have any cash flow at this current time. I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits. Frustrating situation!\",\n",
       "  \"I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits.\"),\n",
       " (1,\n",
       "  'Wing sauce is like water. Pretty much a lot of butter and some hot sauce (franks red hot maybe).  The whole wings are good size and crispy, but for $1 a wing the sauce could be better. The hot and extra hot are about the same flavor/heat.  The fish sandwich is good and is a large portion, sides are decent.',\n",
       "  'The whole wings are good size and crispy, but for $1 a wing the sauce could be better.'),\n",
       " (2,\n",
       "  \"Decent range somewhat close to the city.  The mats are pretty solid; however, the grass range needs to be tended too.  It's like hitting out of US Open type rough...not very amenable to practicing.  Which kind of defeats the purpose of going to a golf range...Still gets 3 stars because the range is lit up at night which is excellent for those of us who are addicted to this amazing game, but are somewhat short on time (having a job kinda sucks sometimes, no?).\",\n",
       "  'Which kind of defeats the purpose of going to a golf range...Still gets 3 stars because the range is lit up at night which is excellent for those of us who are addicted to this amazing game, but are somewhat short on time (having a job kinda sucks sometimes, no?')]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "raw_train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class multiLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,raw_data):\n",
    "        self.data = raw_data\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.load(\"train_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(raw_train_data)\n",
    "train_dataset = multiLabelDataset(raw_train_data[:6500])\n",
    "test_dataset = multiLabelDataset(raw_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scl_data import summarize\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "def my_collect(batch):\n",
    "    batch_size = len(batch)\n",
    "    perm_index = torch.randperm(batch_size)\n",
    "    y_a = torch.LongTensor([item[0] for item in batch])\n",
    "    # print(y_a)\n",
    "    # print(batch)\n",
    "    y_b = y_a[perm_index]\n",
    "    x = [item[1] for item in batch]\n",
    "    # x_perm = [x[perm_index[i]] for i in range(batch_size)]\n",
    "    # s = [summarize(x[i]) for i in range(batch_size)]\n",
    "    s = [item[2] for item in batch]\n",
    "    s_perm = [s[perm_index[i]] for i in range(batch_size)]\n",
    "    s_mix = [s[i] + \"\\n\" + s_perm[i] for i in range(batch_size)]\n",
    "\n",
    "    x_ids = tokenizer(x, padding = 'max_length', max_length = 200, truncation = True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    s_mix_ids = tokenizer(s, padding = 'max_length', max_length = 200, truncation = True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    return x_ids, s_mix_ids, y_a, y_b\n",
    "    \n",
    "    \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = 4,collate_fn = my_collect, num_workers=2, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = 32,collate_fn = my_collect, num_workers=2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from scl_model import scl_model_multi\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "import torch\n",
    "# device = torch.device(args.gpu_ids)\n",
    "device = torch.device(\"cuda:0\")\n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "config.num_labels = 5\n",
    "pretrained_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",config=config)\n",
    "model = scl_model_multi(config,device,pretrained_model,False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt import OpenAIAdam\n",
    "optimizer = OpenAIAdam(model.parameters(),\n",
    "                                  lr=1e-5,\n",
    "                                  schedule='warmup_linear',\n",
    "                                  warmup=0.002,\n",
    "                                  t_total=10000,\n",
    "                                  b1=0.9,\n",
    "                                  b2=0.999,\n",
    "                                  e=1e-08,\n",
    "                                  l2=0.01,\n",
    "                                  vector_l2=True,\n",
    "                                  max_grad_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([train_dataset,test_dataset],\"train_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, recoder, step):\n",
    "    print(\"Evaluation Start======\")\n",
    "    model.eval()\n",
    "    # TP, TN, FN, FP = 0, 0, 0, 0\n",
    "    \n",
    "    # step = 0\n",
    "    bar = tqdm.tqdm(total=len(test_loader))\n",
    "    bar.update(0)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_ids, s_mix_ids, y_a, y_b = batch\n",
    "            seq_ids = x_ids.to(device)\n",
    "            labels = y_a.to(device)\n",
    "            logits = model.predict(seq_ids)\n",
    "            # print(logits)\n",
    "\n",
    "            prediction = torch.argmax(logits, dim = 1)\n",
    "            correct += (prediction == labels).sum()\n",
    "            total += prediction.shape[0]\n",
    "\n",
    "    acc = correct / total\n",
    "    print(\"Acc: \",acc)\n",
    "\n",
    "    recoder.log_test(acc,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]labels: tensor([[2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [3]], device='cuda:0')\n",
      "logits: tensor([[1.2500, 1.2206, 1.2354, 1.1286, 1.2204, 1.2200, 1.2245, 1.2008],\n",
      "        [1.2206, 1.2500, 1.2165, 1.1782, 1.2231, 1.2389, 1.2403, 1.2136],\n",
      "        [1.2354, 1.2165, 1.2500, 1.1258, 1.2297, 1.2123, 1.2208, 1.1957],\n",
      "        [1.1286, 1.1782, 1.1258, 1.2500, 1.1510, 1.1935, 1.1820, 1.2097],\n",
      "        [1.2204, 1.2231, 1.2297, 1.1510, 1.2500, 1.2193, 1.2249, 1.2028],\n",
      "        [1.2200, 1.2389, 1.2123, 1.1935, 1.2193, 1.2500, 1.2402, 1.2265],\n",
      "        [1.2245, 1.2403, 1.2208, 1.1820, 1.2249, 1.2402, 1.2500, 1.2193],\n",
      "        [1.2008, 1.2136, 1.1957, 1.2097, 1.2028, 1.2265, 1.2193, 1.2500]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mask: tensor([[0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.]], device='cuda:0')\n",
      "logits_mask tensor([[0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.]], device='cuda:0')\n",
      "exp_logits: tensor([[3.4903, 3.3893, 3.4398, 3.0913, 3.3885, 3.3871, 3.4025, 3.3228],\n",
      "        [3.3893, 3.4903, 3.3752, 3.2485, 3.3976, 3.4517, 3.4567, 3.3656],\n",
      "        [3.4398, 3.3752, 3.4903, 3.0826, 3.4204, 3.3612, 3.3899, 3.3058],\n",
      "        [3.0913, 3.2485, 3.0826, 3.4903, 3.1613, 3.2987, 3.2610, 3.3525],\n",
      "        [3.3885, 3.3976, 3.4204, 3.1613, 3.4903, 3.3849, 3.4039, 3.3294],\n",
      "        [3.3871, 3.4517, 3.3612, 3.2987, 3.3849, 3.4903, 3.4563, 3.4093],\n",
      "        [3.4025, 3.4567, 3.3899, 3.2610, 3.4039, 3.4563, 3.4903, 3.3850],\n",
      "        [3.3228, 3.3656, 3.3058, 3.3525, 3.3294, 3.4093, 3.3850, 3.4903]],\n",
      "       device='cuda:0', grad_fn=<ExpBackward>)\n",
      "logits: tensor([[1.2500, 1.2206, 1.2354, 1.1286, 1.2204, 1.2200, 1.2245, 1.2008],\n",
      "        [1.2206, 1.2500, 1.2165, 1.1782, 1.2231, 1.2389, 1.2403, 1.2136],\n",
      "        [1.2354, 1.2165, 1.2500, 1.1258, 1.2297, 1.2123, 1.2208, 1.1957],\n",
      "        [1.1286, 1.1782, 1.1258, 1.2500, 1.1510, 1.1935, 1.1820, 1.2097],\n",
      "        [1.2204, 1.2231, 1.2297, 1.1510, 1.2500, 1.2193, 1.2249, 1.2028],\n",
      "        [1.2200, 1.2389, 1.2123, 1.1935, 1.2193, 1.2500, 1.2402, 1.2265],\n",
      "        [1.2245, 1.2403, 1.2208, 1.1820, 1.2249, 1.2402, 1.2500, 1.2193],\n",
      "        [1.2008, 1.2136, 1.1957, 1.2097, 1.2028, 1.2265, 1.2193, 1.2500]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "log_prob: tensor([[-1.9036, -1.9330, -1.9182, -2.0250, -1.9333, -1.9337, -1.9291, -1.9528],\n",
      "        [-1.9442, -1.9148, -1.9484, -1.9866, -1.9418, -1.9260, -1.9245, -1.9512],\n",
      "        [-1.9162, -1.9352, -1.9017, -2.0259, -1.9219, -1.9394, -1.9309, -1.9560],\n",
      "        [-1.9847, -1.9351, -1.9876, -1.8633, -1.9624, -1.9198, -1.9313, -1.9036],\n",
      "        [-1.9360, -1.9333, -1.9267, -2.0054, -1.9064, -1.9371, -1.9315, -1.9536],\n",
      "        [-1.9476, -1.9287, -1.9553, -1.9740, -1.9482, -1.9175, -1.9274, -1.9410],\n",
      "        [-1.9433, -1.9275, -1.9470, -1.9858, -1.9429, -1.9276, -1.9178, -1.9485],\n",
      "        [-1.9549, -1.9421, -1.9601, -1.9460, -1.9529, -1.9292, -1.9364, -1.9057]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "1.587830901145935 1.573899269104004 22.073490142822266\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "class Recoder_multi():\n",
    "    def __init__(self):\n",
    "        self.ce_loss_x = []\n",
    "        self.ce_loss_mix = []\n",
    "        self.scl_loss = []\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "        self.step = []\n",
    "    def log_train(self,ce_loss_x, ce_loss_mix, scl_loss,loss):\n",
    "        self.ce_loss_x.append(ce_loss_x.item())\n",
    "        self.ce_loss_mix.append(ce_loss_mix.item())\n",
    "        self.scl_loss.append(scl_loss.item())\n",
    "        self.loss.append(loss.item())\n",
    "    \n",
    "    def log_test(self,acc,step):\n",
    "        self.acc.append(acc)\n",
    "        self.step.append(step)\n",
    "\n",
    "\n",
    "    def meter(self,step):\n",
    "        print(\"===================================\")\n",
    "        print(\"loss: \",sum(self.loss)/step)\n",
    "        print(\"ce_loss_x: \",sum(self.ce_loss_x)/step)\n",
    "        print(\"ce_loss_mix: \",sum(self.ce_loss_mix)/step)\n",
    "        print(\"scl_loss: \",sum(self.scl_loss)/step)\n",
    "\n",
    "\n",
    "step = 0\n",
    "bar = tqdm.tqdm(total=10000)\n",
    "bar.update(0)\n",
    "best_acc = 0\n",
    "recoder = Recoder_multi()\n",
    "\n",
    "loss_mask = [1,0,0]\n",
    "\n",
    "count = 0\n",
    "begin_eval = False\n",
    "while(step < 10000):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        # optimizer.zero_grad()\n",
    "        ce_loss_x, ce_loss_mix, scl_loss = model(batch)\n",
    "        loss = loss_mask[0] * ce_loss_x + loss_mask[1] * ce_loss_mix + loss_mask[2] * scl_loss\n",
    "        loss.backward()\n",
    "        print(ce_loss_x.item(), ce_loss_mix.item(), scl_loss.item())\n",
    "        break\n",
    "\n",
    "        recoder.log_train(ce_loss_x,ce_loss_mix,scl_loss,loss)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 1 == 0):\n",
    "            # optimizer.step()\n",
    "            step += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (step % 200 == 0):\n",
    "                begin_eval = True\n",
    "\n",
    "            if (step % 10 == 0):\n",
    "                bar.update(10)\n",
    "\n",
    "        # step += 1\n",
    "        \n",
    "        \n",
    "        if begin_eval:\n",
    "            recoder.meter(step)\n",
    "            evaluate_model(model,test_loader,recoder,step)\n",
    "            torch.save(recoder,\"log/run_multi_baseline.pkl\")\n",
    "            \n",
    "            model.train()\n",
    "            begin_eval = False\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 4\n",
    "anchor_count = 2\n",
    "label = torch.LongTensor([2,3,1,1]).unsqueeze(-1)\n",
    "mask = label == label.T\n",
    "mask = mask.repeat(2, 2)\n",
    "\n",
    "logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1),\n",
    "            0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False,  True, False, False, False],\n",
       "        [False,  True, False, False, False,  True, False, False],\n",
       "        [False, False,  True,  True, False, False,  True,  True],\n",
       "        [False, False,  True,  True, False, False,  True,  True],\n",
       "        [ True, False, False, False,  True, False, False, False],\n",
       "        [False,  True, False, False, False,  True, False, False],\n",
       "        [False, False,  True,  True, False, False,  True,  True],\n",
       "        [False, False,  True,  True, False, False,  True,  True]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True, False,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True, False,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True, False,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True, False,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True, False,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False,  True, False, False, False],\n",
       "        [False, False, False, False, False,  True, False, False],\n",
       "        [False, False, False,  True, False, False,  True,  True],\n",
       "        [False, False,  True, False, False, False,  True,  True],\n",
       "        [ True, False, False, False, False, False, False, False],\n",
       "        [False,  True, False, False, False, False, False, False],\n",
       "        [False, False,  True,  True, False, False, False,  True],\n",
       "        [False, False,  True,  True, False, False,  True, False]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "mask * logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[41.8977, 39.9862, 39.9220, 38.5843, 37.1067, 38.1846, 38.1274, 41.6253,\n",
       "         41.9710, 36.4523],\n",
       "        [42.4863, 40.5480, 40.4828, 39.1263, 37.6279, 38.7210, 38.6630, 42.2101,\n",
       "         42.5606, 36.9643],\n",
       "        [41.8212, 39.9132, 39.8490, 38.5138, 37.0389, 38.1148, 38.0577, 41.5493,\n",
       "         41.8943, 36.3856],\n",
       "        [43.9832, 41.9765, 41.9091, 40.5048, 38.9536, 40.0852, 40.0252, 43.6972,\n",
       "         44.0601, 38.2666],\n",
       "        [43.0596, 41.0951, 41.0291, 39.6543, 38.1357, 39.2435, 39.1848, 42.7797,\n",
       "         43.1349, 37.4631],\n",
       "        [40.7969, 38.9357, 38.8731, 37.5705, 36.1317, 37.1813, 37.1257, 40.5317,\n",
       "         40.8683, 35.4945],\n",
       "        [42.4008, 40.4664, 40.4013, 39.0476, 37.5522, 38.6431, 38.5852, 42.1252,\n",
       "         42.4750, 36.8900],\n",
       "        [40.0300, 38.2037, 38.1423, 36.8642, 35.4525, 36.4824, 36.4277, 39.7697,\n",
       "         40.1000, 34.8273],\n",
       "        [43.2007, 41.2298, 41.1635, 39.7842, 38.2607, 39.3721, 39.3132, 42.9199,\n",
       "         43.2763, 37.5859],\n",
       "        [40.7945, 38.9334, 38.8708, 37.5683, 36.1296, 37.1791, 37.1235, 40.5293,\n",
       "         40.8659, 35.4924]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "a = torch.rand([10,120])\n",
    "b = torch.rand([10,120])\n",
    "torch.matmul(torch.norm(a,dim = 1,keepdim=True), torch.norm(b,dim=1,keepdim = True).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "import torch\n",
    "from scl_model import scl_model_multi\n",
    "device = torch.device(0)\n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "config.num_labels = 5\n",
    "pretrained_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",config=config)\n",
    "model_dir = \"model/with_mlp_without_mix_scl_pretrain.pkl\"\n",
    "model = scl_model_multi(config,device,pretrained_model,with_semi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_dir))"
   ]
  }
 ]
}